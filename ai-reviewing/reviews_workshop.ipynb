{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "from perform_review import perform_review, load_paper, reviewer_system_prompt_neg\n",
    "\n",
    "\n",
    "def review_paper(\n",
    "    paper_name,\n",
    "    folder_name,\n",
    "    num_reviews=5,\n",
    "    reviewer_system_prompt=reviewer_system_prompt_neg,\n",
    "):\n",
    "    paper_text = load_paper(os.path.join(folder_name, paper_name + \".pdf\"))\n",
    "\n",
    "    all_reviews = []\n",
    "    for i in range(num_reviews):\n",
    "        review = perform_review(\n",
    "            paper_text,\n",
    "            model=\"gpt-4o-2024-05-13\",\n",
    "            client=openai.OpenAI(),\n",
    "            num_reflections=5,\n",
    "            num_fs_examples=1,\n",
    "            num_reviews_ensemble=5,\n",
    "            temperature=0.1,\n",
    "            reviewer_system_prompt=reviewer_system_prompt,\n",
    "        )\n",
    "        all_reviews.append(review)\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        with open(\n",
    "            f\"{folder_name}/ai_reviews/{timestamp}_review_{i}_workshop_track.txt\", \"w\"\n",
    "        ) as f:\n",
    "            f.write(json.dumps(review, indent=4))\n",
    "        print(f\"{paper_name}: Finished review {i+1}/{num_reviews}\")\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_description = \"\"\"Submitted papers should contain the following four elements:\n",
    "\n",
    "1. A use case that was tackled with deep learning. \n",
    "\n",
    "2. A solution for this type of use case was proposed in the deep learning literature\n",
    "\n",
    "3. A description of the (negative) outcome in the solution. \n",
    "\n",
    "4. An investigation (and ideally an answer) to the question of why it did not work as promised by the deep learning literature. \n",
    "\n",
    "The potential reasons for failure may include but are not limited to data-related issues (e.g., distribution shift, bias, label quality, noisy measurement, quality of simulated data), model limitations (e.g., assumption violations, robustness, interpretability, scalability, representation misalignment), and deployment challenges (e.g., computational demands, hardware constraints). Besides these four points, papers will be assessed on:\n",
    "\n",
    "1. Rigor and transparency in the scientific methodologies employed. \n",
    "\n",
    "2. Novelty and significance of insights.\n",
    "\n",
    "3. Quality of discussion of limitations.\n",
    "\n",
    "4. Reproducibility of results.\n",
    "\n",
    "5. Clarity of writing.\n",
    "\"\"\"\n",
    "\n",
    "review_prompt_ws = \"You are an AI researcher who is reviewing a paper that was submitted to a ML conference workshop.\"\n",
    "\n",
    "review_prompt_ws += (\n",
    "    \"The workshop is called 'I Can't Believe It's Not Better: Challenges in Applied Deep Learning'.\"\n",
    "    + ws_description\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_paper: Finished review 1/1\n"
     ]
    }
   ],
   "source": [
    "reviews = review_paper(\n",
    "    \"raw_paper\",\n",
    "    \"../compositional-regularization\",\n",
    "    num_reviews=1,\n",
    "    reviewer_system_prompt=review_prompt_ws,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_paper: Finished review 1/1\n"
     ]
    }
   ],
   "source": [
    "reviews = review_paper(\n",
    "    \"raw_paper\",\n",
    "    \"../pest-detection\",\n",
    "    num_reviews=1,\n",
    "    reviewer_system_prompt=review_prompt_ws,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_paper: Finished review 1/1\n"
     ]
    }
   ],
   "source": [
    "reviews = review_paper(\n",
    "    \"raw_paper\",\n",
    "    \"../label-noise\",\n",
    "    num_reviews=1,\n",
    "    reviewer_system_prompt=review_prompt_ws,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
